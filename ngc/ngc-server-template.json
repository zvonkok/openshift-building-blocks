{
  "apiVersion": "v1",
  "kind": "Template",
  "labels": {
    "template": "ngc-server-template"
  },
  "message": "The following NGC-inference-server pod has been created: ${NGC_IMAGE}.\n\n",
  "metadata": {
    "annotations": {
      "description": "Template for NGC-inference-server",
      "iconClass": "icon-shadowman",
      "openshift.io/display-name": "NGC",
      "openshift.io/documentation-url": "https://ngc.nvidia.com/registry",         
      "tags": "NVIDIA,machine-learning"
    },
    "name": "ngc-inference-server"
  },
  "objects": [
    {
      "apiVersion": "v1",
      "kind": "Pod",
      "metadata": {
	"name": "${NGC_POD_NAME}",
	"namespace": "nvidia"
      },
      "spec": {
	"restartPolicy": "OnFailure",
	"containers": [
	  {
            "name": "ngc",
            "image": "${NGC_IMAGE}",
            "command": [
              "/bin/bash",
              "-c",
              "trap : TERM INT; sleep infinity & wait"
            ],
            "env": null,
            "volumeMounts": [
              {
		"mountPath": "/dev/shm",
		"name": "dshm"
              }
            ],
            "securityContext": {
              "allowPrivilegeEscalation": false,
              "capabilities": {
		"drop": [
		  "ALL"
		]
              },
              "seLinuxOptions": {
		"type": "nvidia_container_t"
              }
            }
	  }
	],
	"volumes": [
	  {
            "name": "dshm",
            "empytDir": {
              "medium": "Memory"
            }
	  }
	]
      }
    }
  ],
  "parameters": [
    {
      "description": "Which inferenceserver image to use: nvcr.io/nvidia/tensorrtserver:18.09-py3 ...",
      "displayName": "NGC InferenceServer Image",
      "name": "NGC_IMAGE",
      "value": "nvcr.io/nvidia/tensorrtserver:18.09-py3"
    },
    {
      "description": "Choose the display name of the container image: tensorrtserver, inferenceserver ...",
      "displayName": "NGC Pod Name",
      "name": "NGC_POD_NAME",
      "value": "tensorrtserver-18-09-py3"
    }
  ]



}
