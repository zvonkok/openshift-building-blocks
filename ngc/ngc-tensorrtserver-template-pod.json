{
  "apiVersion": "v1",
  "kind": "Template",
  "labels": {
    "template": "ngc-tensorrtserver-template"
  },
  "message": "The following NGC-TensorRT Inference Server pod has been created: ${NGC_IMAGE}.\n\n",
  "metadata": {
    "annotations": {
      "description": "Template for NGC-TensorRT Inference Server",
      "iconClass": "icon-shadowman",
      "openshift.io/display-name": "NGC TensorRT Inference Server",
      "openshift.io/documentation-url": "https://ngc.nvidia.com/registry",
      "tags": "NVIDIA,machine-learning"
    },
    "name": "ngc-tensorrtserver"
  },
  "objects": [
    {
      "kind": "Service",
      "apiVersion": "v1",
      "metadata": {
	"name": "tensorrtserver-service"
      },
      "spec": {
	"selector": {
	  "app": "tensorrtserver"
	},
	"ports": [
	  {
            "name": "http",
            "protocol": "TCP",
            "port": 8000,
            "targetPort": 8000
	  },
	  {
            "name": "grpc",
            "port": 8001,
            "targetPort": 8001
	  },
	  {
            "name": "prometheus",
            "port": 8002,
            "targetPort": 8002
	  }
	],
	"sessionAffinity": "None",
        "type": "ClusterIP"
      }
    },
    {
      "apiVersion": "v1",
      "kind": "Pod",
      "metadata": {
	"name": "tensorrtserver-18-09-py3",
	"labels": {
	  "app": "tensorrtserver"
	}
      },
      "spec": {
	"restartPolicy": "OnFailure",
	"containers": [
	  {
	    "capabilities": {},
            "name": "ngc",
            "image": "nvcr.io/nvidia/tensorrtserver:18.09-py3",
	    "command": ["/bin/bash"],
      "args": ["-c", "/opt/tensorrtserver/bin/trtserver --model-store=/model-store/tensorrtserver/examples/models"],
            "env": null,
	    "ports": [
	      {
		"containerPort": 8000,
		"name": "http"
	      },
	      {
		"containerPort": 8001,
		"name": "grpc"
	      },
	      {
		"containerPort": 8002,
		"name": "prometheus"
	      }
	    ],
     "volumeMounts": [
              {
                "mountPath": "/model-store",
                "name": "model-store"
              },
              {
		"mountPath": "/dev/shm",
		"name": "dshm"
              }
            ],
            "resources": {
                "limits": {
                    "nvidia.com/gpu": "${NUM_GPUS}"
                }
            }
          }
	],
	"volumes": [
    {
      "name": "model-store",
      "persistentVolumeClaim": {
        "claimName": "nfs-model-store"
      }
    },
	  {
            "name": "dshm",
            "empytDir": {
              "medium": "Memory"
            }
	  }
	]
      }
    }
  ],
  "parameters": [
      {
      "description": "How many GPUs to use",
      "displayName": "Number of GPUs",
      "name": "NUM_GPUS",
      "value": "1"
    }
  ]
}
